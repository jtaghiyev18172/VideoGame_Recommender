{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8a391c9c-addc-4046-9ca3-e22cb5d23bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# === 1. Load and preprocess data ===\n",
    "df = pd.read_csv(\"steam-200k.csv\", header=None)\n",
    "df.columns = [\"user_id\", \"game\", \"action\", \"playtime\", \"_\"]\n",
    "df = df[df[\"action\"] == \"play\"].drop(columns=[\"action\", \"_\"])\n",
    "df = df[df[\"playtime\"] > 0]  # optional: clean 0s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c46ec09f-a87d-4e1e-a911-4faba87b6e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply log1p transformation to playtime\n",
    "df[\"log_playtime\"] = np.log1p(df[\"playtime\"])\n",
    "\n",
    "# Create user and game ID maps\n",
    "user_ids = df[\"user_id\"].unique()\n",
    "game_ids = df[\"game\"].unique()\n",
    "user_map = {uid: idx for idx, uid in enumerate(user_ids)}\n",
    "game_map = {gid: idx for idx, gid in enumerate(game_ids)}\n",
    "game_map_rev = {idx: gid for gid, idx in game_map.items()}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "336ab01a-7875-42bc-b3fd-43a14193fc9a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1: RMSE = 0.9008\n",
      "Iteration 2: RMSE = 0.4624\n",
      "Iteration 3: RMSE = 0.3737\n",
      "Iteration 4: RMSE = 0.3276\n",
      "Iteration 5: RMSE = 0.2975\n",
      "Iteration 6: RMSE = 0.2757\n",
      "Iteration 7: RMSE = 0.2592\n",
      "Iteration 8: RMSE = 0.2461\n",
      "Iteration 9: RMSE = 0.2354\n",
      "Iteration 10: RMSE = 0.2264\n"
     ]
    }
   ],
   "source": [
    "# === 2. User-wise train/test split ===\n",
    "def userwise_train_test_split(df, test_size=0.2):\n",
    "    train_rows, test_rows = [], []\n",
    "    for uid, group in df.groupby(\"user_id\"):\n",
    "        if len(group) < 2:\n",
    "            train_rows.append(group)\n",
    "        else:\n",
    "            train, test = train_test_split(group, test_size=test_size, random_state=42)\n",
    "            train_rows.append(train)\n",
    "            test_rows.append(test)\n",
    "    return pd.concat(train_rows), pd.concat(test_rows)\n",
    "\n",
    "df_train, df_test = userwise_train_test_split(df)\n",
    "\n",
    "# === 3. Build user-item matrix ===\n",
    "def build_matrix(df_subset, user_map, game_map, value_col=\"log_playtime\"):\n",
    "    R = np.zeros((len(user_map), len(game_map)))\n",
    "    for _, row in df_subset.iterrows():\n",
    "        u = user_map.get(row[\"user_id\"])\n",
    "        i = game_map.get(row[\"game\"])\n",
    "        if u is not None and i is not None:\n",
    "            R[u, i] = row[value_col]\n",
    "    return R\n",
    "\n",
    "R_train = build_matrix(df_train, user_map, game_map)\n",
    "\n",
    "# === 4. ALS Implementation ===\n",
    "def als(R, num_factors=20, num_iters=10, lambda_reg=0.1):\n",
    "    num_users, num_items = R.shape\n",
    "    U = np.random.normal(scale=1./num_factors, size=(num_users, num_factors))\n",
    "    V = np.random.normal(scale=1./num_factors, size=(num_items, num_factors))\n",
    "\n",
    "    for it in range(num_iters):\n",
    "        for u in range(num_users):\n",
    "            V_i = V[R[u, :] > 0]\n",
    "            R_u = R[u, R[u, :] > 0]\n",
    "            if len(R_u) == 0: continue\n",
    "            A = V_i.T @ V_i + lambda_reg * np.eye(num_factors)\n",
    "            b = V_i.T @ R_u\n",
    "            U[u] = np.linalg.solve(A, b)\n",
    "\n",
    "        for i in range(num_items):\n",
    "            U_u = U[R[:, i] > 0]\n",
    "            R_i = R[R[:, i] > 0, i]\n",
    "            if len(R_i) == 0: continue\n",
    "            A = U_u.T @ U_u + lambda_reg * np.eye(num_factors)\n",
    "            b = U_u.T @ R_i\n",
    "            V[i] = np.linalg.solve(A, b)\n",
    "\n",
    "        # Optional: training RMSE\n",
    "        pred = U @ V.T\n",
    "        mask = R > 0\n",
    "        train_rmse = np.sqrt(((mask * (R - pred))**2).sum() / mask.sum())\n",
    "        print(f\"Iteration {it+1}: RMSE = {train_rmse:.4f}\")\n",
    "\n",
    "    return U, V\n",
    "\n",
    "# === 5. Train ALS ===\n",
    "U, V = als(R_train, num_factors=20, num_iters=10, lambda_reg=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ad7c30df-3b68-4491-8edd-472606ce8577",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üéÆ Top Recommendations for User 0:\n",
      "Overlord: 5.80\n",
      "Half-Life Opposing Force: 5.62\n",
      "Star Wars Jedi Knight Jedi Academy: 5.21\n",
      "Resident Evil 5 / Biohazard 5: 5.19\n",
      "Elite Dangerous: 4.94\n",
      "\n",
      "üìä Test Evaluation:\n",
      "MAE  = 1.9808\n",
      "RMSE = 2.6276\n"
     ]
    }
   ],
   "source": [
    "# === 6. Recommend games for a user ===\n",
    "def recommend(user_index, U, V, R, top_n=5):\n",
    "    scores = U[user_index] @ V.T\n",
    "    scores[R[user_index] > 0] = -np.inf\n",
    "    top_items = np.argsort(scores)[-top_n:][::-1]\n",
    "    return [(game_map_rev[i], scores[i]) for i in top_items]\n",
    "\n",
    "# Example: Recommend for user 0\n",
    "print(\"\\nüéÆ Top Recommendations for User 0:\")\n",
    "recs = recommend(0, U, V, R_train)\n",
    "for game, score in recs:\n",
    "    print(f\"{game}: {score:.2f}\")\n",
    "\n",
    "# === 7. Evaluate on test set (RMSE & MAE) ===\n",
    "def evaluate_on_test(df_test, U, V, user_map, game_map, value_col=\"log_playtime\"):\n",
    "    abs_errors = []\n",
    "    squared_errors = []\n",
    "\n",
    "    for _, row in df_test.iterrows():\n",
    "        uid = row[\"user_id\"]\n",
    "        gid = row[\"game\"]\n",
    "        true_val = row[value_col]\n",
    "\n",
    "        if uid not in user_map or gid not in game_map:\n",
    "            continue\n",
    "\n",
    "        u = user_map[uid]\n",
    "        i = game_map[gid]\n",
    "        pred = np.dot(U[u], V[i])\n",
    "\n",
    "        abs_errors.append(abs(true_val - pred))\n",
    "        squared_errors.append((true_val - pred)**2)\n",
    "\n",
    "    mae = np.mean(abs_errors)\n",
    "    rmse = np.sqrt(np.mean(squared_errors))\n",
    "    print(f\"\\nüìä Test Evaluation:\\nMAE  = {mae:.4f}\\nRMSE = {rmse:.4f}\")\n",
    "    return mae, rmse\n",
    "\n",
    "# Evaluate\n",
    "evaluate_on_test(df_test, U, V, user_map, game_map)\n",
    "\n",
    "\n",
    "\n",
    "def grid_search_als(R_train, df_test, user_map, game_map):\n",
    "    best_rmse = float('inf')\n",
    "    best_mae = float('inf')\n",
    "    best_params_rmse = {}\n",
    "    best_params_mae = {}\n",
    "\n",
    "    num_factors_list = [10, 20]\n",
    "    lambda_list = [0.01, 0.1]\n",
    "    num_iters_list = [5, 10]\n",
    "\n",
    "    print(\"üîç Starting grid search...\\n\")\n",
    "\n",
    "    for num_factors in num_factors_list:\n",
    "        for lambda_reg in lambda_list:\n",
    "            for num_iters in num_iters_list:\n",
    "                print(f\"‚ñ∂ Trying: factors={num_factors}, lambda={lambda_reg}, iters={num_iters}\")\n",
    "                U, V = als(R_train, num_factors=num_factors, num_iters=num_iters, lambda_reg=lambda_reg)\n",
    "                mae, rmse = evaluate_on_test(df_test, U, V, user_map, game_map)\n",
    "\n",
    "                if rmse < best_rmse:\n",
    "                    best_rmse = rmse\n",
    "                    best_params_rmse = {\n",
    "                        \"num_factors\": num_factors,\n",
    "                        \"lambda_reg\": lambda_reg,\n",
    "                        \"num_iters\": num_iters\n",
    "                    }\n",
    "\n",
    "                if mae < best_mae:\n",
    "                    best_mae = mae\n",
    "                    best_params_mae = {\n",
    "                        \"num_factors\": num_factors,\n",
    "                        \"lambda_reg\": lambda_reg,\n",
    "                        \"num_iters\": num_iters\n",
    "                    }\n",
    "\n",
    "    print(\"\\n‚úÖ Grid Search Results:\")\n",
    "    print(f\"üîπ Best RMSE = {best_rmse:.4f} with {best_params_rmse}\")\n",
    "    print(f\"üî∏ Best MAE  = {best_mae:.4f} with {best_params_mae}\")\n",
    "    return best_params_rmse, best_params_mae\n",
    "\n",
    "# best_rmse_params, best_mae_params = grid_search_als(R_train, df_test, user_map,¬†game_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "78750ad3-a818-4c71-a6c0-0ed3e603db35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "225 cold-start games in test set\n"
     ]
    }
   ],
   "source": [
    "train_games = set(df_train[\"game\"])\n",
    "test_games = set(df_test[\"game\"])\n",
    "cold_start_games = test_games - train_games\n",
    "print(f\"{len(cold_start_games)} cold-start games in test set\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c0ceb018-3864-4bd5-995f-2f0a979517dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 cold-start users in test set\n"
     ]
    }
   ],
   "source": [
    "train_users = set(df_train[\"user_id\"])\n",
    "test_users = set(df_test[\"user_id\"])\n",
    "cold_start_users = test_users - train_users\n",
    "print(f\"{len(cold_start_users)} cold-start users in test set\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2539f548-399f-4bb4-8aa0-3f77e274b01e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered Test RMSE: 2.6398\n",
      "Filtered Test MAE : 1.9910\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "# 1. Filter test set to games seen in training\n",
    "known_games = set(df_train[\"game\"])\n",
    "filtered_test_df = df_test[df_test[\"game\"].isin(known_games)].copy()\n",
    "\n",
    "df[\"log_playtime\"] = np.log1p(df[\"playtime\"])\n",
    "\n",
    "# 2. Map user_id and game to indices used in U and V\n",
    "filtered_test_df[\"user_idx\"] = filtered_test_df[\"user_id\"].map(user_map)\n",
    "filtered_test_df[\"game_idx\"] = filtered_test_df[\"game\"].map(game_map)\n",
    "\n",
    "# 3. Predict using U and V\n",
    "actuals, preds = [], []\n",
    "\n",
    "for row in filtered_test_df.itertuples():\n",
    "    u_idx = row.user_idx\n",
    "    i_idx = row.game_idx\n",
    "\n",
    "    if np.isnan(u_idx) or np.isnan(i_idx):\n",
    "        continue  # skip invalid mappings\n",
    "\n",
    "    pred = U[u_idx] @ V[i_idx].T\n",
    "    actual = np.log1p(row.playtime)  # or row.log_playtime if using log scale\n",
    "\n",
    "    preds.append(pred)\n",
    "    actuals.append(actual)\n",
    "\n",
    "# 4. Evaluate\n",
    "rmse = np.sqrt(mean_squared_error(actuals, preds))\n",
    "mae = mean_absolute_error(actuals, preds)\n",
    "\n",
    "print(f\"Filtered Test RMSE: {rmse:.4f}\")\n",
    "print(f\"Filtered Test MAE : {mae:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
